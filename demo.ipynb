{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531467a2-5160-4073-a990-0d81d574b014",
   "metadata": {},
   "source": [
    "## (1) Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9337043-4e7a-4b20-9d89-6c6257245334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from mamba.model import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "# One of:\n",
    "#     'state-spaces/mamba-2.8b-slimpj'\n",
    "#     'state-spaces/mamba-2.8b'\n",
    "#     'state-spaces/mamba-1.4b'\n",
    "#     'state-spaces/mamba-790m'\n",
    "#     'state-spaces/mamba-370m'\n",
    "#     'state-spaces/mamba-130m'\n",
    "pretrained_model_name = 'weights/mamba-370m'\n",
    "\n",
    "# model = Mamba.from_pretrained(pretrained_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"weights/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bdd16d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huzheng/anaconda3/envs/mynet/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = 'weights/mamba-370m'\n",
    "\n",
    "model = Mamba.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db6fe95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArgs(d_model=1024, n_layer=48, vocab_size=50280, d_state=16, expand=2, dt_rank=64, d_conv=4, pad_vocab_size_multiple=8, conv_bias=True, bias=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2efb17-37ad-472b-b029-9567acf17629",
   "metadata": {},
   "source": [
    "## (2) Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b2d62d-0d95-4a3f-bd98-aa37e3f26b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def generate(model,\n",
    "             tokenizer,\n",
    "             prompt: str,\n",
    "             n_tokens_to_gen: int = 50,\n",
    "             sample: bool = True,\n",
    "             top_k: int = 40):\n",
    "    model.eval()\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "    for token_n in tqdm(range(n_tokens_to_gen)):\n",
    "        with torch.no_grad():\n",
    "            indices_to_input = input_ids\n",
    "            next_token_logits = model(indices_to_input)[:, -1]\n",
    "\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        (batch, vocab_size) = probs.shape\n",
    "\n",
    "        if top_k is not None:\n",
    "            (values, indices) = torch.topk(probs, k=top_k)\n",
    "            probs[probs < values[:, -1, None]] = 0\n",
    "            probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "        if sample:\n",
    "            next_indices = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\n",
    "    output_completions = [tokenizer.decode(\n",
    "        output.tolist()) for output in input_ids][0]\n",
    "\n",
    "    return output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee877143-2042-4579-8042-a96db6200517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 50/50 [01:09<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba is the first all-electric motorcycle on the market. It was launched in Japan last summer after the successful release of the first generation Electric Bike. Mamba was created with the goal to combine the qualities of e-bikes with the efficiency of conventional motor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'Mamba is the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d70549-597f-49ca-9185-2184d2576f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:07<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John: Hi!\n",
      "Sally: Hi!\n",
      "John: We were out and had a nice conversation\n",
      "about your book and how it was done.\n",
      "Sally: It definitely was.\n",
      "John: What was that like when you wrote it and the\n",
      "audiences were interested in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'John: Hi!\\nSally:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d419fc9-066b-4818-812c-2f1952528bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, 'The meaning of life is '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b189e6e-6a96-4770-88cf-7c5de22cb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, 'def reverse_string('))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be3afb51-5093-4c64-ac3f-43c2e6b20b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mamba(\n",
       "  (embedding): Embedding(50280, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-47): 48 x ResidualBlock(\n",
       "      (mixer): MambaBlock(\n",
       "        (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "        (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
       "        (x_proj): Linear(in_features=2048, out_features=96, bias=False)\n",
       "        (dt_proj): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm_f): RMSNorm()\n",
       "  (lm_head): Linear(in_features=1024, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6531acc0-b18f-472a-8e99-cee64dd51cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResidualBlock(\n",
       "  (mixer): MambaBlock(\n",
       "    (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "    (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
       "    (x_proj): Linear(in_features=2048, out_features=96, bias=False)\n",
       "    (dt_proj): Linear(in_features=64, out_features=2048, bias=True)\n",
       "    (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efe197-891a-4ab8-8cea-413d1fb1acda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99509b-df7b-4bac-b6a2-669f601ec1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
